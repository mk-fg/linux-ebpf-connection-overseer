#!/usr/bin/env python

# Relevant docs:
#  https://docs.ebpf.io/ebpf-library/libbpf/userspace/
#  https://libbpf.readthedocs.io/en/latest/api.html

import collections as cs, ipaddress as ip, pathlib as pl
import ctypes as ct, socket as so, functools as ft
import os, sys, re, struct, errno, time, json, pwd, signal, select, math, string


class adict(dict):
	def __init__(self, *args, **kws):
		super().__init__(*args, **kws)
		self.__dict__ = self

def sz_repr(sz, _units=list(reversed(
		list((u, 2 ** (i * 10)) for i, u in enumerate('BKMGT')) ))):
	for u, u1 in _units:
		if sz > u1: break
	return f'{sz / u1:.1f}'.removesuffix('.0') + u

def td_repr( td, units_max=1, units_res=None, fmt=None,
		_units=dict(h=3600,m=60,s=1,y=365.2422*86400,mo=30.5*86400,w=7*86400,d=1*86400) ):
	res, s, n_last = list(), td, units_max - 1
	units = sorted(_units.items(), key=lambda v: v[1], reverse=True)
	for unit, us in units:
		if not (val := math.floor(val_raw := s / us)):
			if units_res == unit: break
			continue
		elif val_raw - val > 0.98 * us: val += 1
		if len(res) == n_last or units_res == unit: val, n_last = round(s / us), True
		res.append(f'{val:.0f}{unit}')
		if n_last is True or not (s := s - val * us): break
	if not res: return 'now'
	res = ' '.join(res)
	return res if not fmt else fmt.format(res)

def td_parse(td, _tds = dict(
		w=7*86400, wk=7*86400, week=7*86400,
		d=1*86400, day=1*86400, h=3600, hr=3600, hour=3600,
		m=60, min=60, minute=60, s=1, sec=1, second=1 )):
	'Parse [[HH:]MM:]SS or e.g. 5h30m and such time-deltas to seconds'
	if td is None: return None
	if isinstance(td, (int, float)): return td
	delta, td_re = 0, ( '(?i)^[-+]?' + ''.join( fr'(?P<{k}>\d+{k}s?\s*)?' for k, v in
		sorted(_tds.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True) ) + '$' )
	if not ((m := re.search(td_re, td.strip())) and any(m.groups())):
		h, m, s = (v.strip(': ') for v in ('::' + td).rsplit(':', 2))
		return float(s or 0) + float(m or 0) * 60 + float(h or 0) * 3600
	for k, v in _tds.items():
		if not m.group(k): continue
		delta += v * int(''.join(filter(str.isdigit, m.group(k))) or 1)
	return delta

class TimedCacheDict(cs.UserDict):
	'dict that has values expire and get removed on timeout'
	__slots__, _no_value = ('data', 'ts', 'ts_min', 'timeout', 'bump_on_get'), object()
	timeout_cleanup_slack = 1.5 # oldest-timeout multiplier to cache cleanup
	optional = classmethod(lambda cls,t,**k: cls(t,**k) if t and t > 0 else None)
	def __init__(self, timeout, bump_on_get=True):
		self.ts, self.ts_min = dict(), None
		self.timeout, self.bump_on_get = timeout, bump_on_get
		super().__init__()
	def cache(self, k, v=_no_value):
		ts, get_op = time.monotonic(), v is self._no_value
		if get_op and (ts - self.ts.get(k, 0)) > self.timeout: raise KeyError(k)
		if not get_op or self.bump_on_get: self.ts[k] = ts
		v = super().__getitem__(k) if get_op else super().__setitem__(k, v)
		if not self.ts_min: self.ts_min = ts; return v
		elif get_op or ts - self.ts_min <= self.timeout * self.timeout_cleanup_slack: return v
		for k, ts0 in sorted(self.ts.items(), key=lambda kv: kv[1]):
			if ts - ts0 > self.timeout: self.pop(k, None); del self.ts[k]; continue
			self.ts_min = ts0; break
		else: self.ts_min = None
		return v
	def __iter__(self):
		for k in self.data:
			if k in self: yield k
	def __contains__(self, k):
		return k in self.data and (time.monotonic() - self.ts.get(k, 0)) <= self.timeout
	def __getitem__(self, k): return self.cache(k)
	def __setitem__(self, k, v): return self.cache(k, v)
	def get(self, k, default=None):
		try: return self[k]
		except KeyError: return default

p = lambda *a,**kw: print(*a, **kw, flush=True)
p_err = lambda *a,**kw: print(*a, **kw, file=sys.stderr, flush=True) or 1
inst_fmt = lambda v: f'[{v.__class__.__name__}] {v}'


class CgroupMap:
	'Resolves cgroup_id into cgfs-relative path str efficiently'
	# cgroup ids should be persistent, monotonically increasing
	# There's no need to cache anything but ids, as direntries have them alongside names
	# Does not check for recursively-mounted subdirs - will go into inf-loop there

	def __init__(self, cache, cgfs='/sys/fs/cgroup'):
		self.cache, self.cgfs = cache, cgfs
		self.cg_iter = self.cg_iter_func(); next(self.cg_iter)

	def __enter__(self): return self
	def __exit__(self, *err):
		try: self.cg_iter.send(StopIteration)
		except StopIteration: pass

	def cg_iter_func(self, end=StopIteration):
		'''Infinite iterator yielding relative cgid path str
			from cache or from continued/repeated cgfs scans.'''
		q, cgfs_len = cs.deque(), len(self.cgfs) + 1
		cgid_loop, cgid_get = ..., (yield) or 1
		while cgid_get is not end:
			if cgid_loop == cgid_get: # full scan failed to find cgid
				self.cache[cgid_get] = '' # negative cache - cgroup is likely gone
				while cgid_get in self.cache: cgid_get = yield self.cache[cgid_get]
			cgid_loop = cgid_get; q.append(self.cgfs)
			while q:
				if cgid_get is end: q.popleft(); continue
				pp = q.popleft()
				try:
					for p in os.scandir(pp):
						if not p.is_dir(follow_symlinks=False) or cgid_get is end: continue
						self.cache[cgid := p.inode()] = (cgid_p := p.path[cgfs_len:])
						if cgid_get == cgid:
							while cgid_get in self.cache: cgid_get = yield self.cache[cgid_get]
						q.append(p)
				except OSError: pass # access issues or removed paths

	def get(self, cgid):
		if p := self.cache.get(cgid := int(cgid)): return p
		return self.cg_iter.send(cgid)


class BPFError(Exception): pass

class BPFMap:

	class _rb_struct(ct.Structure): pass
	class _map_info(ct.Structure): _align_, _fields_ = 8, list( (k, ct.c_uint)
		for k in 'type id key value entires flags'.split() ) + [('name', ct.c_char * 16)]
	class _map_batch_opts(ct.Structure): _fields_ = [
		('sz', ct.c_size_t), ('elem_flags', ct.c_ulonglong), ('flags', ct.c_ulonglong) ]
	_rb_handler_t = ct.CFUNCTYPE(ct.c_int, ct.c_void_p, ct.c_void_p, ct.c_size_t)

	_libbpf = None # needed for ringbuf_process_ring asm to process data
	@classmethod
	def libbpf_init(cls):
		if BPFMap._libbpf: return
		lib = BPFMap._libbpf = ct.CDLL('libbpf.so.1', use_errno=True)
		ptr, cint, cuint, cvoid = ct.POINTER, ct.c_int, ct.c_uint, ct.c_void_p
		lib.ring_buffer__new.argtypes = cint, cls._rb_handler_t, cvoid, cvoid
		lib.ring_buffer__new.restype = ptr(cls._rb_struct)
		lib.ring_buffer__poll.argtypes = ptr(cls._rb_struct), cint
		lib.bpf_map_get_info_by_fd.argtypes = cint, ptr(cls._map_info), ptr(cuint)
		lib.bpf_map_lookup_batch.argtypes = ( cint,
			cvoid, cvoid, cvoid, cvoid, ptr(cuint), ptr(cls._map_batch_opts) )

	fd = fd_close = None
	def map_fd(self, fd=None, path=None):
		if self.fd is not None: return self.fd
		if fd is None:
			if not path: raise ValueError('Either bpf-map fd or path must be specified')
			if (fd := self._libbpf.bpf_obj_get(str(path).encode())) < 0:
				raise BPFError(f'Failed bpf-map init from path for [ {path} ]')
			self.fd_close = True
		self.fd = fd; return fd

	def __enter__(self): return self
	def __exit__(self, *err): self.close()
	def close(self):
		if self.fd_close and self.fd is not None: os.close(self.fd); self.fd = None

	def ev_parse( self, ev, o=0,
			_st=struct.Struct('<BQ16s16sHHII16sQQQQ'), _ct_proto='tcp udp x'.split(),
			_fields='ct ns laddr raddr lport rport pid uid comm ns_trx rx tx cg'.split(),
			_ct_af=((so.AddressFamily.AF_INET6, 16), (so.AddressFamily.AF_INET, 4)) ):
		e = adict(zip(_fields, _st.unpack_from(ev, offset=o)))
		if not e.ct: return
		e.proto, (e.af, n) = _ct_proto[(e.ct-1)//2], _ct_af[e.ct%2]
		for k in 'laddr', 'raddr': e[k] = ip.ip_address(e[k][:n])
		e.comm = e.comm.rstrip(b'\0').decode(errors='replace')
		return e

class BPFMapHashTable(BPFMap):
	# There's no good high-level API for opening maps from fds,
	#  so using "low-level" api here, which is same thing but with less hassle.
	# Also trivial to use direct syscalls instead, but libbpf is needed for rbs anyway.

	def __init__(self, fd=None, path=None):
		self.libbpf_init()
		self.fd, info_fn = self.map_fd(fd, path), self._libbpf.bpf_map_get_info_by_fd
		info, n = self._map_info(), ct.c_uint(n0 := ct.sizeof(self._map_info))
		if info_fn(self.fd, ct.byref(info), ct.byref(n)) or n.value < n0:
			raise BPFError(f'libbpf bpf_map_get_info failed [fd={fd} path={path}]')
		self.name, self.n = info.name.decode(), info.entires
		self.ksz, self.vsz = info.key, info.value
		self.batch_flags = self._map_batch_opts(sz=ct.sizeof(self._map_batch_opts))

	def read(self, batch=64):
		# ENOENT shouldn't happen here, but indicates that q goes beyond max_entries
		vals, a, b = list(), ct.pointer(ct.c_void_p()), ct.pointer(ct.c_void_p())
		kb_data, vb_data = (bytearray(n * batch) for n in [self.ksz, self.vsz])
		kb, vb = ct.c_char.from_buffer(kb_data), ct.c_char.from_buffer(vb_data)
		q = ct.c_uint(); kb_p, vb_p, q_p = (ct.pointer(v) for v in [kb, vb, q])
		while c := self.n - len(vals):
			c = q.value = min(batch, c)
			if err := self._libbpf.bpf_map_lookup_batch( self.fd,
					None if not vals else a, b, kb_p, vb_p, q_p, self.batch_flags ):
				if -err != errno.ENOENT: raise BPFError('bpf_map_lookup_batch failed')
			for n in range(q.value):
				if not (ev := self.ev_parse(vb_data, n*self.vsz)): return vals
				vals.append(ev)
			if err or q.value < c: break
			a, b = b, a
		return vals

class BPFMapRingBuffer(BPFMap):

	def __init__(self, fd=None, path=None):
		self.libbpf_init()
		self.updates, self.handler = list(), self._rb_handler_t(self._ev_handler)
		self.rb = self._libbpf.ring_buffer__new(self.map_fd(fd, path), self.handler, None, None)
		if not self.rb: raise BPFError(f'libbpf ring_buffer__new failed [fd={fd} path={path}]')

	def _ev_handler(self, ctx, ev, n):
		return self.updates.append(self.ev_parse(
			bytes((ct.c_char*n).from_address(ev)) )) or 0

	def close(self):
		if self.rb: self._libbpf.ring_buffer__free(self.rb); self.rb = None
		super().close()

	def wait(self, timeout=1.0):
		self.updates.clear()
		if self._libbpf.ring_buffer__poll(
			self.rb, int(timeout*1000) ) <= 0: return () # timeout/error
		return tuple(self.updates)


def ev_conv_fmt_pid_cmd(comm, pid, cut=16, cache=None, _shebangs=set( '''
		python python3 java node perl lua php stack ocamlscript nimrun
		tcl tclsh sh zsh bash nix-shell awk scm guile clisp racket cl sbcl'''.split() )):
	'Gets script name from /proc/<pid>/cmdline for common interpreters'
	if cache is None: cache = dict()
	if cmd := cache.get(pid): return cmd
	if comm.split(None, 1)[0].lower() in _shebangs:
		try: cmd = pl.Path(f'/proc/{pid}/cmdline').read_bytes().split(b'\0')
		except OSError: cmd = list()
		for arg in cmd[1:]:
			if not arg or arg[0] == b'-' or b'=' in arg: cmd.clear(); continue
			if '.' in (arg := arg.decode(errors='replace')) or '/' in arg:
				cmd = (f'{comm}/' if not cmd else '') + arg.rsplit('/')[-1]
				break # cmd can be "python3 -W ignore script.py", so keep prefix if opts are there
		else: cmd = ''
	if cmd and len(cmd) > cut:
		n, cmds = 1, f'{cmd[:cut-1]}+'
		while cmds in cache and cache[cmds] != cmd and n <= 99:
			ns = str(n); cmds = cmds[:cut-1-len(ns)] + f'~{ns}'; n += 1
		cmd, cache[cmds] = cmds, cmd
	cmd = cache[pid] = cmd or comm; return cmd

def ev_conv_fmt_ep(addr, port, proto):
	if addr == '::': addr = '[::]'
	return f'{addr} {port}' + (f'/{proto}' if proto != 'tcp' else '')

def ev_conv_fmt_cg(cgid, cg_map, cache=None, cut=20):
	'Resolves, cuts down, disambiguates and caches cgroup names'
	if not (cgp0 := cg_map.get(cgid)): return
	if cache is None: cache = dict()
	if cgn := cache.get(cgp0): return cgn
	cgs, cgp, cgns = '', cgp0, list()
	cgsx = lambda n: re.sub(r'.(service|scope|slice)$', '', n)
	while True:
		cgn = cgsx(cgp.rsplit('/', 1)[-1]) + cgs
		if len(cgn) > cut: cgn = f'{cgn[:cut-1]}+'
		if cgn in cgns: # looping - generate simple num-prefixed id
			cn = len(chars := string.ascii_letters + string.digits)
			for n in range(9999):
				s = ''
				while n >= cn: s += chars[m := n // cn]; n -= m * cn
				s += chars[n]
				if (cgn := f'{cgns[0][:cut-2]}.{s}') not in cache: break
		else: cgns.append(cgn)
		if (cgp1 := cache.get(cgn)) and cgp1 != cgp0: # same-label conflict
			ps = tuple(map(cgsx, [cgp1, cgp0]))
			if suff := os.path.commonpath(p[::-1] for p in ps): cgp = ps[-1][:-len(suff)-1]
			cgs = f'/{cgn}'
			if '/' in suff: cgs = f'/{cgs}'
			continue
		if not cgp1: cache[cgn], cache[cgp0] = cgp0, cgn
		return cgn

def ev_conv( e, all_data=False, local_ep=False,
		fmt_cmd=None, fmt_ep=None, fmt_cg=None,
		traffic_min=5*2**10, td_reltime=22.5*3600, cache_uid=None, cache_ts=None ):
	ev = adict(ns=e.ns, ns_trx=e.ns_trx)
	if not (cache_ts and (ev_ts := cache_ts.get(e.ns))): # "conn started" time
		if (td := (time.monotonic_ns() - e.ns) / 1e9) < td_reltime:
			ev_ts = time.strftime('%H:%M', time.localtime(time.time() - td))
		else: ev_ts = '{:>5s}'.format(td_repr(td, fmt='-{}'))
		if cache_ts is not None: cache_ts[e.ns] = ev_ts
	if not (cache_uid and (u := cache_uid.get(e.uid))): # username
		try: u = pwd.getpwuid(e.uid).pw_name
		except KeyError: u = str(e.uid)
		if cache_uid is not None: cache_uid[e.uid] = u
	comm = re.sub(r'::+', '-', e.comm)
	if fmt_cmd: comm = fmt_cmd(comm, e.pid)
	if fmt_cg and (cg := fmt_cg(e.cg)) and cg != comm: comm = f'{comm} [{cg}]'
	ep_info = fmt_ep(e.raddr, e.rport, e.proto),
	if local_ep: ep_info = (fmt_ep(e.laddr, e.lport, e.proto),) + ep_info
	ev.line = ' :: '.join([ ev_ts, u, comm, *ep_info,
		f'v {sz_repr(e.rx)} / {sz_repr(e.tx)} ^' if e.rx + e.tx >= traffic_min else '' ])
	if all_data: ev = dict(e, **ev)
	return ev

def json_conv(v):
	if ip_repr := getattr(v, 'compressed', None): return ip_repr
	raise TypeError(f'Non-JSON type {inst_fmt(v)}')

def stream_events_pid_check(pid):
	try: os.kill(pid, 0)
	except PermissionError: pass
	except ProcessLookupError: return False
	return True

def stream_events(cache, rb, dst, ev_proc, replay_check_pids=False, replay_limit=None):
	def _dst_write(ev, ns):
		try:
			dst.write(json.dumps(ev, default=json_conv))
			dst.write('\n'); dst.flush()
		except BrokenPipeError: # pipe closed
			if dst is sys.stdout: os.dup2(os.open(
				os.devnull, os.O_WRONLY ), dst.fileno())
			return True
	with select.epoll(sizehint=2) as poll:
		ns_cache, ns_cutoff, ns = 0, 0, time.monotonic_ns()
		if replay_limit: ns_cutoff = ns - replay_limit * 1_000_000_000
		for ev in sorted(cache.read(), key=lambda ev: ev.ns_trx):
			if (ns_cache := ev.ns_trx) < ns_cutoff: continue
			if replay_check_pids and not stream_events_pid_check(ev.pid): continue
			if _dst_write(ev_proc(ev), ns): return
		if not rb: return # one-off cache dump
		poll.register(dst_fd := dst.fileno(), 0)
		poll.register(rb.fd, select.EPOLLIN)
		while True:
			if not (ev_fds := poll.poll(maxevents=1)): continue
			if ev_fds[0][0] == dst_fd: break # dst pipe closed
			for ev in rb.wait(0.1): # shouldn't actually need to wait
				if ev.ns_trx <= ns_cache or ev.ns_trx < ns_cutoff: continue
				if replay_check_pids: # only needed for initial queue here
					if replay_check_pids is True: replay_check_pids = time.monotonic()
					elif time.monotonic() - replay_check_pids > 30: replay_check_pids = False
					if not stream_events_pid_check(ev.pid): continue
				if _dst_write(ev_proc(ev), time.monotonic_ns()): return
			if not ev_fds[0][1] & select.EPOLLIN: break # rb fd closed


def main(argv=None):
	import argparse, textwrap
	dd = lambda text: re.sub( r' \t+', ' ',
		textwrap.dedent(text).strip('\n') + '\n' ).replace('\t', '  ')
	conf_str = dd('''
		# Time-delta values can be a number of seconds, [[HH:]MM:]SS or e.g. 5h30m.
		# Setting *-cut, cache-cg-labels or cache-pid-cmd to negative vals disables adding those.
		replay-window=4h # ignore pre-buffered events older than specified time-delta
		replay-skip-dead-pids=0 # set =1 to only replay old info on pids that are still around
		local-addr-port=0 # add local addr/port endpoint info before remote one
		cg-label-cut=20 # longer cgroup names are cut to that, like "app-flatpak-net.lut+"
		script-name-cut=16 # limit detected interpreted script names to this length
		conn-traffic-min=5120 # min traffic to display on connections - to avoid silly 0 numbers
		use-reltime-after=22h30m # when to start showing relative time instead of daytime
		cache-cg-paths=4h # time-delta to forget old cgroup id-name mappings
		cache-cg-labels=30m # used for displayed short/disambiguated cgroup labels
		cache-users=4h # time-delta for user id-name cache (from /etc/passwd, nss, etc)
		cache-conn-ts=1h # cache for formatted connection timestamps
		cache-pid-cmd=3h # cache for interpreted script names from /proc/<pid>/cmdline
		extra-ev-data=0 # same as -E/--extra-ev-data command-line option''')
	parser = argparse.ArgumentParser(
		formatter_class=argparse.RawTextHelpFormatter, description=dd('''
			Script to monitor and forward eBPF events to UI components as json-lines.
			By default checks systemd service env vars,
				uses map sockets from there and outputs stream of events to stdout.
			Alternative is to use -p/--pin option to open/use bpffs-pinned maps.
			Streams events from pre-populated "conn_map"
				hash-table first, and "updates" ring-buffer events after that.
			Replayed old event times are calculated using monotonic clock offset
				from current wall-clock time, so don't take system suspend into account.'''),
		epilog=dd(f'''
			Optional tweaks for -c/--conf or env-vars (e.g. LECO_REPLAY_WINDOW=1d8h):
				{"\n\t\t\t\t".join(conf_str.splitlines())}''') + ' ')
	parser.add_argument('-f', '--fifo', metavar='path', help=dd('''
		FIFO path to use to output stream of events instead of stdout.
		Must be pre-existing, will be reused when other side closes it,
			always sending all stored connection table events after re-opening.'''))
	parser.add_argument('-p', '--pin', metavar='path', help=dd('''
		Top-level path under which all eBPF objects are pinned.
		This script will only access objects under "map" subdirectory.
		Typically under /sys/fs/bpf/ mountpoint. Might require privileged access.'''))
	parser.add_argument('-E', '--extra-ev-data', action='store_true',
		help='Pass through all extra event info for debugging, not just the necessary stuff.')
	parser.add_argument('-C', '--dump-cache', action='store_true',
		help='Only dump/stream data from eBPF conn_map hash-table and exit.')
	parser.add_argument('-c', '--conf', metavar='space-separated-opts', help=dd('''
		Space-separated list of optional parameter overrides. Takes priority over env vars.
		Output from -h/--help option should have a full list of these, same key=value format.'''))
	opts = parser.parse_args(sys.argv[1:] if argv is None else argv)

	conf, conf_parse = adict(), dict()
	for ls, opt in (conf_str.splitlines(), False), ((opts.conf or '').split(), True):
		for line in ls:
			if not (m := re.match(r'(\S+)=(\S+)', line.strip())):
				if opt: parser.error(f'Unrecognized key=value option format: {line!r}')
				continue
			k, v = m[1].lower().replace(*'-_'), m[2]
			if not opt:
				conf_parse[k] = td_parse if re.search(r'\d+([smhd]$|:)', v) else int
				if ev := os.environ.get(f'leco_{k}'.upper()): v = ev
			elif k not in conf: parser.error(f'Unrecognized option {k!r} in {line!r}')
			conf[k] = conf_parse[k](v)
	if opts.extra_ev_data: conf.extra_ev_data = True

	if not opts.pin:
		sd_pid, sd_fds = (
			os.environ.get(f'LISTEN_{k}', '') for k in 'PID FDNAMES'.split() )
		if not sd_pid.isdigit() or int(sd_pid) != os.getpid():
			parser.error('systemd env LISTEN_PID does not match this process')
		sd_fds = (
			adict((k[:-4], 3+n) for n, k in enumerate(sd_fds.split(':')) if k.endswith('__v1'))
			if all(re.search(r'__v\d$', k) for k in sd_fds.split(':')) # versioned fds
			else adict((k, 3 + n) for n, k in enumerate(sd_fds.split(':'))) )
		bpf_rb = BPFMapRingBuffer(sd_fds.updates)
		bpf_cache = BPFMapHashTable(sd_fds.conn_map)
		if sd_sock := os.environ.get('NOTIFY_SOCKET'):
			if sd_sock[0] not in ('/', '@'): parser.error('Unsupported sd_notify socket')
			if sd_sock[0] == '@': sd_sock = '\0' + sd_sock[1:]
			with so.socket(so.AF_UNIX, so.SOCK_DGRAM | so.SOCK_CLOEXEC) as s:
				s.connect(sd_sock); s.sendall(b'READY=1')

	else:
		p_maps = pl.Path(opts.pin) / 'maps'
		bpf_rb = BPFMapRingBuffer(path=p_maps / 'updates')
		bpf_cache = BPFMapHashTable(path=p_maps / 'conn_map')

	signal.signal(signal.SIGINT, signal.SIG_DFL)
	with ( bpf_rb as rb, bpf_cache as cache,
			CgroupMap(cache=TimedCacheDict(conf.cache_cg_paths)) as cg_map ):
		if opts.dump_cache: rb = None
		fmt_cmd = (
			None if (n := conf.script_name_cut) <= 0 or (c := conf.cache_pid_cmd) < 0
			else ft.partial(ev_conv_fmt_pid_cmd, cut=n, cache=TimedCacheDict.optional(c)) )
		fmt_cg = ( None if (n := conf.cg_label_cut) <= 0 or (c := conf.cache_cg_labels) < 0
			else ft.partial(ev_conv_fmt_cg, cg_map=cg_map, cut=n, cache=TimedCacheDict.optional(c)) )
		stream_kws = dict(
			cache=cache, rb=rb,
			replay_limit=td_parse(conf.replay_window),
			replay_check_pids=bool(conf.replay_skip_dead_pids),
			ev_proc=ft.partial( ev_conv,
				all_data=bool(conf.extra_ev_data), local_ep=bool(conf.local_addr_port),
				fmt_ep=ev_conv_fmt_ep, fmt_cmd=fmt_cmd, fmt_cg=fmt_cg,
				traffic_min=conf.conn_traffic_min, td_reltime=conf.use_reltime_after,
				cache_uid=TimedCacheDict.optional(conf.cache_users),
				cache_ts=TimedCacheDict.optional(conf.cache_conn_ts, bump_on_get=False) ))
		if not opts.fifo: return stream_events(dst=sys.stdout, **stream_kws)
		while dst := open(os.open(opts.fifo, os.O_WRONLY), 'w'):
			try: stream_events(dst=dst, **stream_kws)
			finally:
				try: dst.close()
				except BrokenPipeError: pass

if __name__ == '__main__': sys.exit(main())
